{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOTLuqq1foX5"
   },
   "source": [
    "# Neural Machine Translation with Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVU5B1TG-QWk",
    "outputId": "a02b2342-ed46-4beb-ef8b-0f092d3f3b9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JUQd58gfoYB"
   },
   "source": [
    "### What is Attention?\n",
    "\n",
    "Attention is an interface between the encoder and decoder that provides the decoder with information from every encoder hidden state. With this setting, the model is able to selectively focus on useful parts of the input sequence and hence, learn the alignment between them. This helps the model to cope effectively with long input sentences ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "256Tm3C8foYC",
    "outputId": "c286dc58-c5ba-4647-96d0-ddc1c6f65936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chart-studio\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/ce/330794a6b6ca4b9182c38fc69dd2a9cbff60fd49421cb8648ee5fee352dc/chart_studio-1.1.0-py3-none-any.whl (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 2.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from chart-studio) (1.15.0)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from chart-studio) (4.4.1)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from chart-studio) (1.3.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from chart-studio) (2.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->chart-studio) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->chart-studio) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->chart-studio) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->chart-studio) (2020.12.5)\n",
      "Installing collected packages: chart-studio\n",
      "Successfully installed chart-studio-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install chart-studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "4EMgtenZfoYD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import string\n",
    "\n",
    "import chart_studio.plotly\n",
    "import chart_studio.plotly as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "#%plotly.offline.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akOWiboXfoYE"
   },
   "source": [
    "### As in case of any NLP task, after reading the input file, we perform the basic cleaning and preprocessing as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dRfLc50foYE"
   },
   "source": [
    "**The Dataset :** We need a dataset that contains English sentences and their Portuguese translations which can be freely downloaded from this [link](http://www.manythings.org/anki/). Download the file fra-eng.zip and extract it. On each line, the text file contains an English sentence and its French translation, separated by a tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "TPdfCUNJfoYE"
   },
   "outputs": [],
   "source": [
    "file_path = './drive/MyDrive/pol.txt' # please set the path according to your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fzuJpO4foYF",
    "outputId": "80afac30-4b44-4314-e266-e543cf278998"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Will you join us?\\tCzy dołączysz do nas?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #237669 (CK) & #580598 (Bilberry)',\n",
       " 'Will you join us?\\tPrzyłączycie się do nas?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #237669 (CK) & #4719046 (Ceresnya)',\n",
       " 'Will you take it?\\tWeźmiesz to?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3738699 (CK) & #3749525 (gin)',\n",
       " 'You abandoned me.\\tOpuściłeś mnie.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3374731 (CK) & #3817518 (liori)',\n",
       " 'You already paid.\\tJuż zapłaciłeś.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2254931 (CK) & #4550568 (jeedrek)',\n",
       " 'You already paid.\\tJuż zapłaciłaś.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2254931 (CK) & #5868591 (BeataB)',\n",
       " 'You always cheat.\\tZawsze oszukujesz.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2254932 (CK) & #4550579 (jeedrek)',\n",
       " 'You are a genius.\\tJesteś geniuszem.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1895825 (CK) & #3698112 (jeedrek)',\n",
       " 'You are too late.\\tJesteś za późno.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1582108 (Spamster) & #1588343 (TopMan)',\n",
       " 'You can blame me.\\tMożesz mnie obwinić.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2057767 (CK) & #6602381 (princes21)']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = open(file_path, encoding='UTF-8').read().strip().split('\\n')\n",
    "lines[5000:5010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StSWCudGfoYF",
    "outputId": "63742815-03cc-4f57-f5fe-3c4ad047c4f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of records:  40465\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of records: \",len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "21HVoXm8foYG"
   },
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "remove_digits = str.maketrans('', '', string.digits) # Set of all digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GB-IDy5ofoYG"
   },
   "source": [
    "### Function to preprocess English sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tdQlzyLUfoYG"
   },
   "outputs": [],
   "source": [
    "def preprocess_eng_sentence(sent):\n",
    "    '''Function to preprocess English sentence'''\n",
    "    sent = sent.lower() # lower casing\n",
    "    sent = re.sub(\"'\", '', sent) # remove the quotation marks if any\n",
    "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
    "    sent = sent.translate(remove_digits) # remove the digits\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(\" +\", \" \", sent) # remove extra spaces\n",
    "    sent = '<start> ' + sent + ' <end>' # add <start> and <end> tokens\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZZnCCEJfoYH"
   },
   "source": [
    "### Function to preprocess Polish sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zkXB0MgyfoYH"
   },
   "outputs": [],
   "source": [
    "def preprocess_port_sentence(sent):\n",
    "    '''Function to preprocess Portuguese sentence'''\n",
    "    sent = re.sub(\"'\", '', sent) # remove the quotation marks if any\n",
    "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
    "    #sent = re.sub(\"[२३०८१५७९४६]\", \"\", sent) # remove the digits\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(\" +\", \" \", sent) # remove extra spaces\n",
    "    sent = '<start> ' + sent + ' <end>' # add <start> and <end> tokens\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEHKpzFifoYH"
   },
   "source": [
    "### Generate pairs of cleaned English and Polish sentences with start and end tokens added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYsCIACqfoYI",
    "outputId": "dadb99a9-2b2a-4ecc-cb2c-87ca9e1c2aa3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<start> czy dołączysz do nas <end>', '<start> Will you join us <end>'],\n",
       " ['<start> przyłączycie się do nas <end>', '<start> Will you join us <end>'],\n",
       " ['<start> weźmiesz to <end>', '<start> Will you take it <end>'],\n",
       " ['<start> opuściłeś mnie <end>', '<start> You abandoned me <end>'],\n",
       " ['<start> już zapłaciłeś <end>', '<start> You already paid <end>'],\n",
       " ['<start> już zapłaciłaś <end>', '<start> You already paid <end>'],\n",
       " ['<start> zawsze oszukujesz <end>', '<start> You always cheat <end>'],\n",
       " ['<start> jesteś geniuszem <end>', '<start> You are a genius <end>'],\n",
       " ['<start> jesteś za późno <end>', '<start> You are too late <end>'],\n",
       " ['<start> możesz mnie obwinić <end>', '<start> You can blame me <end>']]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate pairs of cleaned English and Portuguese sentences\n",
    "sent_pairs = []\n",
    "for line in lines:\n",
    "    sent_pair = []\n",
    "    eng = line.rstrip().split('\\t')[1]\n",
    "    port = line.rstrip().split('\\t')[0]\n",
    "    eng = preprocess_eng_sentence(eng)\n",
    "    sent_pair.append(eng)\n",
    "    port = preprocess_port_sentence(port)\n",
    "    sent_pair.append(port)\n",
    "    sent_pairs.append(sent_pair)\n",
    "sent_pairs[5000:5010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CppN3gmOfoYI"
   },
   "source": [
    "### Create a class to map every word to an index and vice-versa for any given vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KntNhypZfoYI"
   },
   "outputs": [],
   "source": [
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LvqGS0K7foYJ"
   },
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3qkNLNofoYJ"
   },
   "source": [
    "### Tokenization and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VJUpBFVlfoYJ"
   },
   "outputs": [],
   "source": [
    "def load_dataset(pairs, num_examples):\n",
    "    # pairs => already created cleaned input, output pairs\n",
    "\n",
    "    # index language using the class defined above    \n",
    "    inp_lang = LanguageIndex(en for en, ma in pairs)\n",
    "    targ_lang = LanguageIndex(ma for en, ma in pairs)\n",
    "    \n",
    "    # Vectorize the input and target languages\n",
    "    \n",
    "    # English sentences\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en, ma in pairs]\n",
    "    \n",
    "    # Marathi sentences\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in ma.split(' ')] for en, ma in pairs]\n",
    "    \n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gUxExoBwfoYK"
   },
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(sent_pairs, len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr-rrK4qfoYK"
   },
   "source": [
    "### Creating training and validation sets using an 99-01 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Z-JjQB5foYK",
    "outputId": "c543ed94-fcde-483c-84b9-997f07d787e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40060, 40060, 405, 405)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.01, random_state = 101)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Zs3KBZQIfoYL"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6-MFVAwfoYL"
   },
   "source": [
    "We'll be using GRUs instead of LSTMs as we only have to create one state and implementation would be easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20a6tN7NfoYL"
   },
   "source": [
    "### Create GRU units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6k6GpzhofoYM"
   },
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True, \n",
    "                                   recurrent_activation='sigmoid', \n",
    "                                   recurrent_initializer='glorot_uniform')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXAF1R8jfoYM"
   },
   "source": [
    "### The next step is to define the encoder and decoder network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY-nMaI9foYM"
   },
   "source": [
    "The input to the encoder will be the sentence in English and the output will be the hidden state and cell state of the GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PZkfqkvvfoYM"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_QDKXPxfoYN"
   },
   "source": [
    "The next step is to define the decoder. The decoder will have two inputs: the hidden state and cell state from the encoder and the input sentence, which actually will be the output sentence with a token appended at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "rQDc-XX1foYN"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6Nk5ttIfoYN"
   },
   "source": [
    "Create encoder and decoder objects from their respective classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zC8uVsfSfoYO"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fOuPYexfoYO"
   },
   "source": [
    "### Define the optimizer and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0_E5XnNofoYO"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPXBseFybUuS"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8e-JIojfoYP"
   },
   "source": [
    "### Training the Model\n",
    "To train the model copy training_attention.py here from the drive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_ZEaA_hfoYP"
   },
   "source": [
    "### Restoring the latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4WVd1oqwfoYQ",
    "outputId": "9d59ea4c-60c3-4cec-f958-17f66e6d7c7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4a72460450>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(\"./drive/MyDrive/attention_checkpoints\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2q_d12ZafoYQ"
   },
   "source": [
    "### Inference setup and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "C-Tsrm1CfoYQ"
   },
   "outputs": [],
   "source": [
    "def evaluate(inputs, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    \n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = ''\n",
    "    for i in inputs[0]:\n",
    "        if i == 0:\n",
    "            break\n",
    "        sentence = sentence + inp_lang.idx2word[i] + ' '\n",
    "    sentence = sentence[:-1]\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONdTiT259Rzj",
    "outputId": "5e09a344-853e-49ca-8d3e-f54758a25cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 3.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4) (1.15.0)\n",
      "Collecting singledispatch\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/d1/6a9e922826e03f5af7bf348cfb75bcb0bc4c67e19c36805c2545f34427e5/singledispatch-3.6.2-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.4-cp37-none-any.whl size=1436383 sha256=4175b3de2dee9341b798a7e53279eb3d84fdce39503e1dae4e530819587bc895\n",
      "  Stored in directory: /root/.cache/pip/wheels/4b/c8/24/b2343664bcceb7147efeb21c0b23703a05b23fcfeaceaa2a1e\n",
      "Successfully built nltk\n",
      "Installing collected packages: singledispatch, nltk\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "Successfully installed nltk-3.4 singledispatch-3.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_Yr8_s3F-anb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BWVZigqzH4Xi"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.nist_score import sentence_nist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "JXeg4TzBqraD"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "# gleu_score = []\n",
    "bleu_score = []\n",
    "# nist_score = []\n",
    "sent_len = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ghgt1tp-foYQ"
   },
   "source": [
    "### Function to predict (translate) a randomly selected test point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "FAZ0BupsfoYR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict_random_val_sentence():\n",
    "    actual_sent = ''\n",
    "    for k in range(0,30):\n",
    "      actual_sent=\" \"\n",
    "      random_input = input_tensor_val[k]\n",
    "      random_output = target_tensor_val[k]\n",
    "      random_input = np.expand_dims(random_input,0)\n",
    "      result, sentence, attention_plot = evaluate(random_input, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "      # print(result,sentence)\n",
    "      print('Input: {}'.format(sentence[8:-6]))\n",
    "      print('Predicted translation: {}'.format(result[:-6]))\n",
    "      for i in random_output:\n",
    "          if i == 0:\n",
    "              break\n",
    "          actual_sent = actual_sent + targ_lang.idx2word[i] + ' '\n",
    "      actual_sent = actual_sent[8:-7]\n",
    "      print('Actual translation: {}'.format(actual_sent))\n",
    "      attention_plot = attention_plot[:len(result.split(' '))-2, 1:len(sentence.split(' '))-1]\n",
    "      sentence, result = sentence.split(' '), result.split(' ')\n",
    "      sentence = sentence[1:-1]\n",
    "      result = result[:-2]\n",
    "      print(\"Result is\",result)\n",
    "      reference = [actual_sent.split()]\n",
    "      sent_len.append(len(reference[0]))\n",
    "      candidate = result\n",
    "      print(reference,candidate)\n",
    "      bleu_score_i = sentence_bleu(reference,candidate)\n",
    "      print('BLEU score -> {}'.format(bleu_score_i))\n",
    "      bleu_score.append(bleu_score_i)\n",
    "\n",
    "\n",
    "    # use plotly to generate the heat map\n",
    "    # trace = go.Heatmap(z = attention_plot, x = sentence, y = result, colorscale='greens')\n",
    "    # data=[trace]\n",
    "    # iplot(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NuJTEtvPfoYR",
    "outputId": "2d77ed6d-d169-4ec0-a5ca-3f49b1ab2ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: oto nasza szkoła\n",
      "Predicted translation: This is our school \n",
      "Actual translation:  That is our school\n",
      "Result is ['This', 'is', 'our', 'school']\n",
      "[['That', 'is', 'our', 'school']] ['This', 'is', 'our', 'school']\n",
      "BLEU score -> 8.636168555094496e-78\n",
      "Input: ona się odchudza\n",
      "Predicted translation: She is dieting \n",
      "Actual translation:  Shes dieting\n",
      "Result is ['She', 'is', 'dieting']\n",
      "[['Shes', 'dieting']] ['She', 'is', 'dieting']\n",
      "BLEU score -> 1.384292958842266e-231\n",
      "Input: chcę żebyście poszli z nami\n",
      "Predicted translation: I want the two of us with us \n",
      "Actual translation:  I want you to come with us\n",
      "Result is ['I', 'want', 'the', 'two', 'of', 'us', 'with', 'us']\n",
      "[['I', 'want', 'you', 'to', 'come', 'with', 'us']] ['I', 'want', 'the', 'two', 'of', 'us', 'with', 'us']\n",
      "BLEU score -> 9.170599044431425e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:523: UserWarning:\n",
      "\n",
      "\n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "\n",
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:523: UserWarning:\n",
      "\n",
      "\n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "\n",
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:523: UserWarning:\n",
      "\n",
      "\n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: wyglądają na wyczerpanych\n",
      "Predicted translation: Does I look at all \n",
      "Actual translation:  They look exhausted\n",
      "Result is ['Does', 'I', 'look', 'at', 'all']\n",
      "[['They', 'look', 'exhausted']] ['Does', 'I', 'look', 'at', 'all']\n",
      "BLEU score -> 1.2183324802375697e-231\n",
      "Input: nie umiem otworzyć tej walizki\n",
      "Predicted translation: I cant open this suitcase \n",
      "Actual translation:  I cant figure out how to open this suitcase\n",
      "Result is ['I', 'cant', 'open', 'this', 'suitcase']\n",
      "[['I', 'cant', 'figure', 'out', 'how', 'to', 'open', 'this', 'suitcase']] ['I', 'cant', 'open', 'this', 'suitcase']\n",
      "BLEU score -> 3.8804806708023324e-78\n",
      "Input: on codziennie ogląda telewizję\n",
      "Predicted translation: He watches TV every day \n",
      "Actual translation:  He watches TV every day\n",
      "Result is ['He', 'watches', 'TV', 'every', 'day']\n",
      "[['He', 'watches', 'TV', 'every', 'day']] ['He', 'watches', 'TV', 'every', 'day']\n",
      "BLEU score -> 1.0\n",
      "Input: nigdy nie byłem nieobecny w szkole\n",
      "Predicted translation: Ive never been absent from school \n",
      "Actual translation:  Ive never been absent from school\n",
      "Result is ['Ive', 'never', 'been', 'absent', 'from', 'school']\n",
      "[['Ive', 'never', 'been', 'absent', 'from', 'school']] ['Ive', 'never', 'been', 'absent', 'from', 'school']\n",
      "BLEU score -> 1.0\n",
      "Input: tom by nigdy czegoś takiego nie zrobił\n",
      "Predicted translation: Tom would never do such a thing didnt do that \n",
      "Actual translation:  Tom would never do anything like that\n",
      "Result is ['Tom', 'would', 'never', 'do', 'such', 'a', 'thing', 'didnt', 'do', 'that']\n",
      "[['Tom', 'would', 'never', 'do', 'anything', 'like', 'that']] ['Tom', 'would', 'never', 'do', 'such', 'a', 'thing', 'didnt', 'do', 'that']\n",
      "BLEU score -> 0.2777619034011791\n",
      "Input: być może mam coś dla ciebie\n",
      "Predicted translation: I have something for you \n",
      "Actual translation:  I may have something for you\n",
      "Result is ['I', 'have', 'something', 'for', 'you']\n",
      "[['I', 'may', 'have', 'something', 'for', 'you']] ['I', 'have', 'something', 'for', 'you']\n",
      "BLEU score -> 0.5789300674674098\n",
      "Input: właśnie skończyłem malować dom\n",
      "Predicted translation: I was just finished reading the house \n",
      "Actual translation:  I just finished painting the house\n",
      "Result is ['I', 'was', 'just', 'finished', 'reading', 'the', 'house']\n",
      "[['I', 'just', 'finished', 'painting', 'the', 'house']] ['I', 'was', 'just', 'finished', 'reading', 'the', 'house']\n",
      "BLEU score -> 1.04198122363916e-154\n",
      "Input: miałem bardzo dobry czas\n",
      "Predicted translation: I had a very good time \n",
      "Actual translation:  I had a very good time\n",
      "Result is ['I', 'had', 'a', 'very', 'good', 'time']\n",
      "[['I', 'had', 'a', 'very', 'good', 'time']] ['I', 'had', 'a', 'very', 'good', 'time']\n",
      "BLEU score -> 1.0\n",
      "Input: nie wszystko jest czarnobiałe\n",
      "Predicted translation: All it is in love with baseball \n",
      "Actual translation:  Not everything is black and white\n",
      "Result is ['All', 'it', 'is', 'in', 'love', 'with', 'baseball']\n",
      "[['Not', 'everything', 'is', 'black', 'and', 'white']] ['All', 'it', 'is', 'in', 'love', 'with', 'baseball']\n",
      "BLEU score -> 1.1200407237786664e-231\n",
      "Input: potrzebuję pieniędzy żeby kupić tomowi prezent\n",
      "Predicted translation: I need money to buy a present \n",
      "Actual translation:  I need money to buy Tom a present\n",
      "Result is ['I', 'need', 'money', 'to', 'buy', 'a', 'present']\n",
      "[['I', 'need', 'money', 'to', 'buy', 'Tom', 'a', 'present']] ['I', 'need', 'money', 'to', 'buy', 'a', 'present']\n",
      "BLEU score -> 0.6129752413741056\n",
      "Input: jesteś szalony\n",
      "Predicted translation: Youre insane \n",
      "Actual translation:  Youre mad\n",
      "Result is ['Youre', 'insane']\n",
      "[['Youre', 'mad']] ['Youre', 'insane']\n",
      "BLEU score -> 1.5319719891192393e-231\n",
      "Input: powinnaś powiedzieć tomowi że nie lubisz tego robić\n",
      "Predicted translation: You ought to tell Tom that you dont like doing that \n",
      "Actual translation:  You ought to tell Tom that you dont like doing that\n",
      "Result is ['You', 'ought', 'to', 'tell', 'Tom', 'that', 'you', 'dont', 'like', 'doing', 'that']\n",
      "[['You', 'ought', 'to', 'tell', 'Tom', 'that', 'you', 'dont', 'like', 'doing', 'that']] ['You', 'ought', 'to', 'tell', 'Tom', 'that', 'you', 'dont', 'like', 'doing', 'that']\n",
      "BLEU score -> 1.0\n",
      "Input: tom przyszedł do szkoły bardzo późno tego ranka\n",
      "Predicted translation: Tom came to school after work this morning \n",
      "Actual translation:  Tom got to school very late this morning\n",
      "Result is ['Tom', 'came', 'to', 'school', 'after', 'work', 'this', 'morning']\n",
      "[['Tom', 'got', 'to', 'school', 'very', 'late', 'this', 'morning']] ['Tom', 'came', 'to', 'school', 'after', 'work', 'this', 'morning']\n",
      "BLEU score -> 9.696727898117387e-155\n",
      "Input: on nie ma problemu ze wspinaniem się na drzewa\n",
      "Predicted translation: He has no problem with electronics \n",
      "Actual translation:  He has no trouble climbing trees\n",
      "Result is ['He', 'has', 'no', 'problem', 'with', 'electronics']\n",
      "[['He', 'has', 'no', 'trouble', 'climbing', 'trees']] ['He', 'has', 'no', 'problem', 'with', 'electronics']\n",
      "BLEU score -> 5.775353993361614e-78\n",
      "Input: czy tom się z kimś spotykał\n",
      "Predicted translation: Does Tom helping us \n",
      "Actual translation:  Was Tom dating anyone\n",
      "Result is ['Does', 'Tom', 'helping', 'us']\n",
      "[['Was', 'Tom', 'dating', 'anyone']] ['Does', 'Tom', 'helping', 'us']\n",
      "BLEU score -> 1.2882297539194154e-231\n",
      "Input: przyjdź szybko\n",
      "Predicted translation: Come quickly \n",
      "Actual translation:  Come quickly\n",
      "Result is ['Come', 'quickly']\n",
      "[['Come', 'quickly']] ['Come', 'quickly']\n",
      "BLEU score -> 1.491668146240062e-154\n",
      "Input: kim jest ten mężczyzna z którym rozmawiałeś\n",
      "Predicted translation: Whos the man of you talking to \n",
      "Actual translation:  Who is the man that you were talking with\n",
      "Result is ['Whos', 'the', 'man', 'of', 'you', 'talking', 'to']\n",
      "[['Who', 'is', 'the', 'man', 'that', 'you', 'were', 'talking', 'with']] ['Whos', 'the', 'man', 'of', 'you', 'talking', 'to']\n",
      "BLEU score -> 6.227170448085464e-155\n",
      "Input: przesadziliśmy\n",
      "Predicted translation: Youre losing \n",
      "Actual translation:  We exaggerated\n",
      "Result is ['Youre', 'losing']\n",
      "[['We', 'exaggerated']] ['Youre', 'losing']\n",
      "BLEU score -> 0\n",
      "Input: czy nie kochasz tego uczucia\n",
      "Predicted translation: Dont you love it \n",
      "Actual translation:  Dont you just love it\n",
      "Result is ['Dont', 'you', 'love', 'it']\n",
      "[['Dont', 'you', 'just', 'love', 'it']] ['Dont', 'you', 'love', 'it']\n",
      "BLEU score -> 1.0497255803085492e-154\n",
      "Input: dziś nie będzie padał śnieg\n",
      "Predicted translation: It wont snow today \n",
      "Actual translation:  Its not going to snow today\n",
      "Result is ['It', 'wont', 'snow', 'today']\n",
      "[['Its', 'not', 'going', 'to', 'snow', 'today']] ['It', 'wont', 'snow', 'today']\n",
      "BLEU score -> 5.780789590099596e-155\n",
      "Input: pies był umierający\n",
      "Predicted translation: The dog was dying \n",
      "Actual translation:  The dog was dying\n",
      "Result is ['The', 'dog', 'was', 'dying']\n",
      "[['The', 'dog', 'was', 'dying']] ['The', 'dog', 'was', 'dying']\n",
      "BLEU score -> 1.0\n",
      "Input: musimy wynająć pomieszczenie na nasze przyjęcie\n",
      "Predicted translation: We must pay the room to our party \n",
      "Actual translation:  We need to rent a room for our party\n",
      "Result is ['We', 'must', 'pay', 'the', 'room', 'to', 'our', 'party']\n",
      "[['We', 'need', 'to', 'rent', 'a', 'room', 'for', 'our', 'party']] ['We', 'must', 'pay', 'the', 'room', 'to', 'our', 'party']\n",
      "BLEU score -> 7.1958300848837144e-155\n",
      "Input: tom jest mimem\n",
      "Predicted translation: Tom is being unfair \n",
      "Actual translation:  Tom is a mime\n",
      "Result is ['Tom', 'is', 'being', 'unfair']\n",
      "[['Tom', 'is', 'a', 'mime']] ['Tom', 'is', 'being', 'unfair']\n",
      "BLEU score -> 9.53091075863908e-155\n",
      "Input: tom nie chciał ze mną rozmawiać\n",
      "Predicted translation: Tom didnt want to speak to me \n",
      "Actual translation:  Tom didnt want to talk to me\n",
      "Result is ['Tom', 'didnt', 'want', 'to', 'speak', 'to', 'me']\n",
      "[['Tom', 'didnt', 'want', 'to', 'talk', 'to', 'me']] ['Tom', 'didnt', 'want', 'to', 'speak', 'to', 'me']\n",
      "BLEU score -> 0.488923022434901\n",
      "Input: mam ochotę wysłać tomowi list\n",
      "Predicted translation: I feel like to give Tom a letter \n",
      "Actual translation:  I feel like sending Tom a letter\n",
      "Result is ['I', 'feel', 'like', 'to', 'give', 'Tom', 'a', 'letter']\n",
      "[['I', 'feel', 'like', 'sending', 'Tom', 'a', 'letter']] ['I', 'feel', 'like', 'to', 'give', 'Tom', 'a', 'letter']\n",
      "BLEU score -> 7.508645449069235e-78\n",
      "Input: na podróż poślubną pojechali do włoch\n",
      "Predicted translation: Theres a trip will come to Italy \n",
      "Actual translation:  They went to Italy for their honeymoon\n",
      "Result is ['Theres', 'a', 'trip', 'will', 'come', 'to', 'Italy']\n",
      "[['They', 'went', 'to', 'Italy', 'for', 'their', 'honeymoon']] ['Theres', 'a', 'trip', 'will', 'come', 'to', 'Italy']\n",
      "BLEU score -> 6.968148412761692e-155\n",
      "Input: na ścianie wisi kalendarz\n",
      "Predicted translation: There is a ship on the wall \n",
      "Actual translation:  Theres a calendar hanging on the wall\n",
      "Result is ['There', 'is', 'a', 'ship', 'on', 'the', 'wall']\n",
      "[['Theres', 'a', 'calendar', 'hanging', 'on', 'the', 'wall']] ['There', 'is', 'a', 'ship', 'on', 'the', 'wall']\n",
      "BLEU score -> 5.395774370246974e-78\n"
     ]
    }
   ],
   "source": [
    "predict_random_val_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jy3Z9zYh3p1l",
    "outputId": "10d52026-4ced-4a37-ee4a-2c105ac537ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8787228516981653e-78"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bleu_score) / len(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yA7pQD6h6DU7",
    "outputId": "4852d8e1-b9d5-44c3-b19a-4d1bb4a4c256"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_len),len(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vAqIiW2OpGo"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/sent_len.txt', 'w') as testwritefile:\n",
    "    testwritefile.write(str(sent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGMuRFI9PetP"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/bleu_Score.txt', 'w') as testwritefile:\n",
    "    testwritefile.write(str(bleu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnIPOcR_pSOO"
   },
   "outputs": [],
   "source": [
    "def predict_sentence(s):\n",
    "    # s = preprocess_eng_sentence(s)\n",
    "    # input = np.array([inp_lang.word2idx[s] for s in s.split(' ')] , dtype=\"int32\")\n",
    "    input = s\n",
    "    random_input = np.pad(input, (0, 40 - len(input)))\n",
    "    random_input = np.expand_dims(random_input,0)\n",
    "    result, sentence, attention_plot = evaluate(random_input, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "    # print('Predicted translation: {}'.format(result[:-6]))\n",
    "    return result[:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3AsIyvv1FmGd",
    "outputId": "574fadab-a296-4371-ff7f-4aadf53ab86d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(405, 49)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "anJsm0Xs-deZ",
    "outputId": "5a47da74-dce9-49e8-8a3a-da53e0647755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: oto nasza szkoła\n",
      "Predicted translation: This is our school \n"
     ]
    }
   ],
   "source": [
    "predict_sentence(input_tensor_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCCydgTF--Na",
    "outputId": "24acc084-8cab-45fb-c30d-c2e1b349f85a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_gleu('here is our school' , 'This is our school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hUi5nl0sWZxO",
    "outputId": "0c0a072e-16bd-4216-bca8-4127c7ac35e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4927319453258576"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_nist('here is our school' , 'This is our school', n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iXc6prPwW0z7",
    "outputId": "6774c4dc-5583-48b0-a1a9-d94ff925a957"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36858666333480733"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "corpus_bleu('That is our school', 'This is our school' , smoothing_function=smoothie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5Z1B03hXCw_"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "smoothie = SmoothingFunction().method4\n",
    "bleu_score = 0\n",
    "nist = 0\n",
    "gleu = 0\n",
    "for i in range(0,len(input_tensor_val)):\n",
    "  predicted = predict_sentence(input_tensor_val[i])\n",
    "  actual_sent = \"\"\n",
    "  for i in target_tensor_val[i]:\n",
    "          if i == 0:\n",
    "              break\n",
    "          actual_sent = actual_sent + targ_lang.idx2word[i] + ' '\n",
    "  actual_sent = actual_sent[8:-7]\n",
    "  # print(\"Required : \"+actual_sent)\n",
    "  bleu_score +=  sentence_bleu(actual_sent , predicted , smoothing_function=smoothie)\n",
    "  nist += sentence_nist(actual_sent , predicted , n =5)\n",
    "  gleu += corpus_gleu([actual_sent] , [predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tlQs8CaXaQU"
   },
   "outputs": [],
   "source": [
    "actual_sent = \"\"\n",
    "for i in target_tensor_val[0]:\n",
    "        if i == 0:\n",
    "            break\n",
    "        actual_sent = actual_sent + targ_lang.idx2word[i] + ' '\n",
    "actual_sent = actual_sent[8:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pUyUkqACukjF",
    "outputId": "3347b55b-41b0-4030-bdca-0a182828128b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'That is our school'"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdlK2Y4Yuf6S",
    "outputId": "cf359af1-1589-481b-941f-8a2b041da971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: oto nasza szkoła\n",
      "Predicted translation: This is our school \n"
     ]
    }
   ],
   "source": [
    "predicted = predict_sentence(input_tensor_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "PiXZBjdbwtoV",
    "outputId": "5a842504-9ed7-478c-8398-76305df65d1b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'This is our school'"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sfxe0ahNwwOT",
    "outputId": "9768ce4e-c1c1-45b2-8e50-817c1bb261e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32362446102383025"
      ]
     },
     "execution_count": 89,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_score / 405\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xupb5K8hxCU9",
    "outputId": "bf9fd08d-26aa-4f48-c7de-97a53944f0c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012180688452118156"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gleu / 405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rVu5KiaPxjya",
    "outputId": "5db088f6-21dd-4243-e72e-f39d7854d78f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19613899235090604"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nist / 405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZieFZPBxntV",
    "outputId": "5f73888a-1ad5-4bba-fb13-402a7e60a204"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20270143721078623"
      ]
     },
     "execution_count": 88,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_nist(\"Hello World I am human\" , \"Hello World I am human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dy-70udw6Quy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt_attention.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
